# 爬虫的相关知识

`因为没有找到现成的笔记，就凭着自己的印象写了，可能会出错，如果报错了就去视频中找答案`

### 1.requests模块

```python
使用requests模块可以向网站发送请求
基本格式如下：
import requests
requests.get(url=url)	发送一个get请求
requests.post(url=url)	发送一个post请求
对于get请求来说，为了应对一些反爬机制，我们可能会添加请求头，附加数据，以及代理
对于post请求来说，我们也会带一些请求头，附加数据还有代理，所以最终的格式可能是这样的
requests.get(url=url,headers=headers,para=para,proxies=proxies)
requests.post(url=url,headers=headers,data=data,proxies=proxies)


proxies的格式是这样的：
proxies={"http":"111.231.94.44:8888"}

对于post请求来说，有一些反爬措施会在网站中添加cookies，这种cookies有的可能是在html的页面中，也有可能在点击提交的按钮的div中，当然所提交的data数据中还有一些隐藏的数据，类似于_aef这样以下划线开头的参数，一般在html中，找不到就ctrl+f进行全局搜索

除了post和get请求以外对于cookie来说，requests模块中还给我们提供了一个session对象，这个对象会保存cookie，所以当发现请求头中有cookie的时候可以直接使用这个方式来进行操作

对于requests的请求内容还有一些参数需要注意一下，将请求的内容复制给a
a.statu			#返回请求的状态吗，200就是成功拿到请求
a.text			#以字符串的形式返回整张页面
a.content		#以二进制的形式返回整张页面（一般是图像或者音频数据）
```

### 2.使用requests模块的单线程+多任务的异步协程模式

```python
import asyncio
import time
import aiohttp  #因为requests不支持异步，所以使用aiohttp这个模块来进行异步的使用
from lxml import etree
urls={
'www.1.com',
'www.2.com'
}

async def get_request(url):
    async with aiohttp.ClientSession() as s:
        async with await s.get(url=url) as response:
            page_test=await response.text()
    return page_text
def parse(task):
    page_text=task.result()
    tree=etree.HTML(page_text)
    tree.xpath('//li/text()')

tasks=[]
for url in urls:
    c=get_request(url)					#绑定对象
    task=asyncio.ensure_future(c)		#创建一个任务
    task.add_done_callback(parse)	#创建一个回调，任务进行完成之后就会调用，执行回调函数，一般是对爬取的对象进行解析
    tasks.append(task)

loop=assyncio.get_event_loop()		#事件循环对象
loop.run_until_compltet(asyncio.wait(tasks))		#提交任务才会进行执行
```

### 3.数据解析

```python
数据解析有很多种
常用的有正则，bs4,xpath
更为常用的是xpath,以及正则
对于xpath来说，更多的是对请求到的页面进行数据的解析
对于正则来说，更多的对字符串进行匹配

正则：
使用(.*?)基本就可以解决所有的问题了，还有一点需要注意，如下：
re.find(html,正则的法则,re.R)		#re.R这个参数是让.可以匹配\n

xpath：##注意：xpath的表达式中不能出现tbody否则解析不到
from lxml import etree
tree=etree.HTML(response)
tree.xpath("/html/head/title")
tree.xpath("//title")
tree.xpath("/html/body//p")	#简单总结就是/用来找儿子//用来找后代
tree.xpath("//div[@class='song']")	#属性查找
tree.xpath("//li[7]")		#标签加数值进行定位
tree.xpath("//a[@id='feng'/text()]")[0]	#text的方法取到的是一个列表，需要0来取值
tree.xpath("//a[@id='feng']/@href")		#直接去属性
```

### 4.selenium的使用

```python
这是一个自动化的的模块，对于爬虫来说，是一个功能非常强大的模块，他会将很多动态加载的内容直接显示出来，可见即可爬。但是这个模块有一个缺点那就是慢，所以这个是爬虫的杀手锏，当是在找不到某些数据的时候，可以用这个模块来实现爬取
```

```python
from selenium import webdriver
from time import sleep
bro = webdriver.Chrome(executable_path="chromedriver.exe")		#印象中在括号里可以什么都不加，如果启动不了就加这个参数
sleep(1)
bro.get('http://125.36.6.84:84/xk/')#使用selenium发起一个get请求

serch_input=bro.find_element_by_id('key')
search_input.send.keys(mac pro)

btn=bro.find_element_by_xpath('//*[@id='search']/div/div[2]/button')
btn.click()

page_text=bro.page_source
#这个参数用来把此时的页面整个赋值

bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
#这个是用来执行js的代码，是让滚轮滑动一个屏幕大小的页面
sleep(2)
bro.quit()
```

```python
对于一些特殊情况下的selenium的使用
对于需要拖动的情况
from selenium import webdriver
from time import sleep
from selenium.webdriver import ActionChains		#这是一个运动链
bro = webdriver.Chrome(executable_path="chromedriver.exe")	


bro.get('https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')
bro.switch_to.frame('iframeResult')		#引号里面是iframe的id值
#这个iframe可以让里面的参数在默认情况下的selenium爬取不到，北航的官网也用了这样一个反爬机制，使用上面这个参数就可以进行爬取
div_tay=bro.find_element_by_id('draggable')

action=ActionChains(bro)
action.click_and_hold(div_tag)

for i in range(5):
    action.move_by_offest(17,5).perform()	#这里的perform个人感觉是直接渲染
    sleep(0.5)
    
    
action.release()
sleep(3)
bro.quit()
```

```python
一些特殊的用法
bro.save_screenshot('main.png')		#对当前页面进行截图
code_img_tag=bro.find_element_by_xpath('//*[id="loginForm"]/div/ul[2]/li[4]/div/div/div[3]/img')
location=code_img_tag.location		#返回图像的位置
size=code_img_tag.size			#返回图像的大小
####上面两个参数都是字典的形式

from PIL import Image
rangle=(int(location['x']),int(location['y']),int(location['x']+size['width']),int(location['y'])+size['hight'])		#得到图像大小的一个矩形

i=Image.open('./main.png')
frame=i.crop(rangle)
frame.save('code.png')		#以上为对图像的感兴趣的区域进行提取

action=ActionChains(bro)
for a in all_list:
    x=a[0]
    y=a[1]
    action.move_to_element_with_offset(code_img_tag,x,y).click().perform()
    #以上时移动到目的区域并点击一下
    #move_to_element_with_offset的第一个参数个人认为是改变一下坐标系，让感兴趣的区域的左上角作为原点
```

```python
selenium的一些其他用法
1.无头浏览器（静默模式）
from selenimu.webdriver.chrome.options import Options

chrome_options=Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')

driver=webdriver.Chrome(chrome_options=chrome_options)
#以上为静默浏览器的配置方法
```

#### 5.scrapy的使用